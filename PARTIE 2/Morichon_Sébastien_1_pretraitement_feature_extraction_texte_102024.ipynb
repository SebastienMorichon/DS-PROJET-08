{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linear_sum_assignment\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m     23\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import adjusted_rand_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Chargement et nettoyage des données ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Télécharger les ressources nécessaires de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données du fichier CSV\n",
    "data = pd.read_csv('Flipkart/flipkart_com-ecommerce_sample_1050.csv')\n",
    "data = data[[\"uniq_id\", \"product_name\", \"description\", \"product_category_tree\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage et extraction des catégories\n",
    "# Extraire uniquement la première catégorie d'une chaîne de catégories séparées par \">>\"\n",
    "data[\"product_category_tree\"].replace(to_replace=r'[\\[\"\\]]', value=\"\", regex=True, inplace=True)\n",
    "category = data[\"product_category_tree\"].str.split(\" >> \", expand=True)\n",
    "category.rename(columns={0: \"Categorie\"}, inplace=True)\n",
    "data = data.join(category[\"Categorie\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage des textes\n",
    "# Fonction de nettoyage pour retirer la ponctuation, les chiffres et les majuscules\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_description'] = data['description'].apply(clean_text)\n",
    "data['cleaned_product_name'] = data['product_name'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation, suppression des stop-words et lemmatisation\n",
    "def preprocess_text(column):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def process(text):\n",
    "        tokens = word_tokenize(text)  # Tokeniser le texte\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    return column.apply(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application du prétraitement au texte nettoyé\n",
    "data['processed_description'] = preprocess_text(data['cleaned_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['processed_description'], data['Categorie'], test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Vectorisation Bag of Words (BoW) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_bow = CountVectorizer()\n",
    "X_train_bow = vectorizer_bow.fit_transform(X_train).toarray()\n",
    "X_test_bow = vectorizer_bow.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Vectorisation TF-IDF ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer_tfidf = TfidfVectorizer(min_df=0.1, max_df=0.85)\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train).toarray()\n",
    "X_test_tfidf = vectorizer_tfidf.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Word2Vec ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement d'un modèle Word2Vec sur les données d'entraînement\n",
    "sentences = [sentence.split() for sentence in X_train]\n",
    "w2v_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour obtenir les embeddings Word2Vec d'une liste de phrases\n",
    "def get_word2vec_embeddings(text_list, model):\n",
    "    return np.array([np.mean([model.wv[word] for word in text.split() if word in model.wv], axis=0) for text in text_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v = get_word2vec_embeddings(X_train, w2v_model)\n",
    "X_test_w2v = get_word2vec_embeddings(X_test, w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Universal Sentence Encoder (USE) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "X_train_use = use_model(X_train.tolist()).numpy()\n",
    "X_test_use = use_model(X_test.tolist()).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- BERT ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour extraire les embeddings de BERT\n",
    "def get_bert_embeddings(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return torch.mean(outputs.last_hidden_state, dim=1).squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bert = np.array([get_bert_embeddings(sentence, tokenizer, bert_model) for sentence in X_train])\n",
    "X_test_bert = np.array([get_bert_embeddings(sentence, tokenizer, bert_model) for sentence in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- KMeans et calcul d'ARI ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_clustering(X_train, X_test, y_test, method_name, num_clusters=7):\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "    kmeans.fit(X_train)\n",
    "    y_pred = kmeans.predict(X_test)\n",
    "\n",
    "    # Encodage des catégories réelles pour le calcul de l'ARI\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_test_encoded = label_encoder.fit_transform(y_test)\n",
    "\n",
    "    # Calcul de l'ARI (Adjusted Rand Index) pour mesurer la qualité du clustering\n",
    "    ari = adjusted_rand_score(y_test_encoded, y_pred)\n",
    "\n",
    "    # Création et affichage de la matrice de confusion avec appariement optimal des clusters\n",
    "    conf_mat = confusion_matrix(y_test_encoded, y_pred)\n",
    "    row_ind, col_ind = linear_sum_assignment(-conf_mat)\n",
    "    cluster_to_label = {cluster: label_encoder.inverse_transform([label])[0] for cluster, label in zip(col_ind, row_ind)}\n",
    "    y_pred_mapped = [cluster_to_label[label] for label in y_pred]\n",
    "\n",
    "    new_conf_mat = confusion_matrix(y_test, y_pred_mapped)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(new_conf_mat, annot=True, cmap='Blues', fmt='d')\n",
    "    plt.title(f'Matrice de confusion ({method_name}) - KMeans ({X_train.shape[1]} features)')\n",
    "    plt.savefig(f'Texte - Matrice de confusion ({method_name}).png')\n",
    "    plt.show()\n",
    "\n",
    "    # Réduction de dimension pour la visualisation avec t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    X_test_tsne = tsne.fit_transform(X_test)\n",
    "\n",
    "    # Visualisation KMeans avec les labels prédits\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.scatterplot(x=X_test_tsne[:, 0], y=X_test_tsne[:, 1], hue=y_pred, palette='viridis')\n",
    "    plt.title(f'KMeans ({method_name}) - Visualisation avec Labels Prédits (t-SNE)')\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.legend(title=\"Clusters prédits\")\n",
    "    plt.savefig(f'Texte - TSNE - K-Means Labels prédits ({method_name}).png')\n",
    "    plt.show()\n",
    "\n",
    "    # Visualisation KMeans avec les labels réels\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.scatterplot(x=X_test_tsne[:, 0], y=X_test_tsne[:, 1], hue=y_test, palette='viridis')\n",
    "    plt.title(f'KMeans ({method_name}) - Visualisation avec Labels Réels (t-SNE)')\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.legend(title=\"Labels réels\")\n",
    "    plt.savefig(f'Texte - TSNE - K-Means Labels réels ({method_name}).png')\n",
    "    plt.show()\n",
    "\n",
    "    return ari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualisation avec PCA et t-SNE ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pca_tsne(X_train, y_train, method_name):\n",
    "    pca = PCA(n_components=2)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=0, init='random')\n",
    "    X_train_tsne = tsne.fit_transform(X_train)\n",
    "    \n",
    "    # Visualisation des clusters obtenus par PCA\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.scatterplot(x=X_train_pca[:, 0], y=X_train_pca[:, 1], hue=y_train, palette='viridis')\n",
    "    plt.title(f'PCA - {method_name}')\n",
    "    plt.show()\n",
    "\n",
    "    # Visualisation des clusters obtenus par t-SNE\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.scatterplot(x=X_train_tsne[:, 0], y=X_train_tsne[:, 1], hue=y_train, palette='viridis')\n",
    "    plt.title(f't-SNE - {method_name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Application sur chaque méthode ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = {\n",
    "    'Bag of Words': (X_train_bow, X_test_bow),\n",
    "    'TF-IDF': (X_train_tfidf, X_test_tfidf),\n",
    "    'Word2Vec': (X_train_w2v, X_test_w2v),\n",
    "    'BERT': (X_train_bert, X_test_bert),\n",
    "    'USE': (X_train_use, X_test_use)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for method_name, (X_train_vec, X_test_vec) in methods.items():\n",
    "    # Calcul de l'ARI et affichage des résultats pour chaque méthode\n",
    "    ari = kmeans_clustering(X_train_vec, X_test_vec, y_test)\n",
    "    results.append({'Technique': method_name, 'ARI': ari})\n",
    "    visualize_pca_tsne(X_train_vec, y_train, method_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Résultats finaux ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des ARI pour chaque méthode\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Technique', y='ARI', data=results_df, palette='Blues_d')\n",
    "plt.title('Comparaison des résultats ARI pour différentes techniques de traitement de texte')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
